<html>
	<head>
		<style type="text/css">
			body{font-family: sans-serif;}
			p{display: inline-block;}
			img{display: block;}
			.container{width: 90%;position absolute;margin: auto;}
			.title{position: relative;width: 90%;margin: auto;text-align: center;font-weight: bold;font-size: 18px;padding: 1%;}
			.section{position: relative;width: 90%;margin: auto;padding: 2%;}
			.subsection{position: relative; width: 98%;text-align: justify;padding: 10px;}
			.heading{position: relative; width: 98%;text-align: left;font-size: 14px;font-weight: bold;}
			.text{width: 95%;font-size: 12px;text-align: justify;padding: 10px 0px 10px 0px;}
			.authors{position: relative;width: 80%;margin: auto;padding: 2%;font-style: italic;text-align: center;font-size: 12px;}
			.image{width: 95%;font-size: 12px;text-align: left;}
		</style>
	</head>
	<body>
		<div class="container">
			<div class="title" style="color:DodgerBlue;">ACTIVITY RECOGNITION USING  CELL-PHONE ACCCELEROMETER</div>

			<div class="authors">

				<!-- Start edit here  -->
				<p>E PONGBA KONYAK, Roll No.: 150102018, Branch: ECE</p>; &nbsp; &nbsp;
				<p>GIRIJA SHANKAR KUANR, Roll No.: 150102020, Branch: ECE</p>; &nbsp; &nbsp;
				<!-- Stop edit here -->

			</div>


			<div class="section">
				<div class="heading">ABSTRACT</div>
				<div class="text">

					<!-- Start edit here  -->
					Mobile devices are becoming increasingly sophisticated and they have many sensors like GPS sensors, vison sensors(i.e.,camera), audio sensors(i.e.,microphones),light sensors,temperature sensors,direction sensors and acceleration sensors(i.e.,accelerometers).In this project we describe and evaluate a system that uses phone based accelerometers to perform activity recognition, a task which involves identifying the physical activity a user is performing.To implement our system we collected labeled accelerometer data from 10 users as they performed daily activities such as walking ,jogging,climbing stairs,sitting and standing,and then aggrgated this time series data into examples that suumerise the user activity over 10 second intervals. We then used the reslting training data to induce a predictive model for activity recognition.Our work has a wide range of application including automatic customization of the model of the mobile device's behaviour based upon a user's activity(e.g.,sending calls directly to voicemail if a user is jogging).
					<!-- Stop edit here -->

				</div>
			</div>

			<div class="section">
				<div class="heading">1. Introduction</div>
				<div class="text">

					<!-- Start edit here  -->
					Mobile devices such as cellular phones and music players have recently begun to incorporate diverse and powerful sensors.
					<!-- Stop edit here -->
					<div class="image">

						<!-- Start edit here  -->
						
						<img src="Pictures/example.jpg
							  
							  
							  " alt="This text displays when the image is umavailable" width="300px" height=""/>
						<!-- Stop edit here -->

					</div>
					The goal of our WISDM(Wireless Data Mining) project is to explore the research issues related to mining sensor data from these powerful mobile devices and to build useful aplications.In this project we explore the use of one of these sensors, the accelerometer,in order to identify the activity that the user is performing.Nowadays many android phones contain tri-axial accelerometers that measure acceleration in all three spatial dimensions.These accelerometers are also capable of detecting the orientation of the device which can provide useful information for activity recognition.For e.g we can automatically moniter a user's activity level and generate daily,weekly and monthly activity reports.The activity information can also be used to automatically customize the behaviour of the mobile phone e.g,music could automatically be selected to match the activity or send calls directly to voice mail when the user is exercising etc.<br>
					
			
			        </div>

				<div class="subsection">
					<div class="heading">1.1 Introduction to Problem</div>
					<div class="text">

				                <!-- Start edit here  -->When we watch a person, it is easy for us to tell what activity they are performing even if we have never seen them in the past. This is because our brains are already trained to understand human activities. When viewing the activity, the brain compares it to thousands of activities it has memorized and pops out the one that matches. Similarly, a computer (or phone) can identify the activity we are performing based on activities we have trained it to identify.
						
						<!-- Stop edit here -->

					</div>
				</div>

				<div class="subsection">
					<div class="heading">1.2 Figure</div>
					<div class="image">

						<!-- Start edit here  -->
						
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading">1.3 Literature Review</div>
					<div class="text">

						<!-- Start edit here  -->
						Write something here.
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading">1.4 Proposed Approach</div>
					<div class="text">

						<!-- Start edit here  -->
						<ul>
						  <li>Raw time series data must be transform into examples as standard classifications algorithms cannot be directly applied to raw time-series accelerometer data.</li>
						  <li>Data is divided into 10 second segments and then generated features that were based on the 200 readings contained within each 10 second segment.</li>
						  <li>Duration of each segment is referred to as the example duration(ED).</li>
						<ul>
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading">1.5 Report Organization</div>
					<div class="text">

						<!-- Start edit here  -->
						Write something here.
						<!-- Stop edit here -->

					</div>
				</div>
			</div>

			<div class="section">
				<div class="heading">2. Proposed Approach</div>
				<div class="text">

					<!-- Start edit here  -->
					<ul>
					  <li>Firstly, we will place the cell phone(i.e, accelerometer) in the front pocket of the pant.</li>
					  <li>Then informative features based on raw accelerometer readings are generated.</li>
					  <li>Each reading contains X,Y and Z values corresponding to the three axes/dimensions</li>
					  For this purposes,
              			<ol>
						  <li>Z-axis:captures the forward movement of the leg.</li>
						  <li>Y-axis:captures the upward and downward motion.</li>
						  <li>X-axis:captures the horizontal movement of the user's leg.</li>
						  These axes are relative to a user.
						</ol>
						<div class="image">

							<!-- Start edit here  -->
						
							<img src="Pictures/motion.png" alt="This text displays when the image is umavailable" width="300px" height=""/>
							<!-- Stop edit here -->

						</div>
					  <li>The informative features are described below,with the number of features generated for each feature-type noted in brackets:
					    <ol>
						   <li>Average[3]: Average acceleration (for each axis)</li> 
						   <li>Standard Deviation[3]: Standard deviation (for each axis)</li>
						   <li>Average Absolute Difference[3]: Average absolute difference between the value of each of the 200 readings within the ED and the mean value over those 200 values (for each axis)</li>  
						   <li>Average Resultant Acceleration[1]: Average of the square roots of the sum of the values of  each axis squared(xi^2 + yi^2 + zi^2) over the ED</li>
                           <li>Time Between Peaks[3]: Time in milliseconds between peaks in the sinusoidal waves associated with most activities (for each axis)</li>
						   <li>Binned Distribution[30]: We determine the range of values for each axis (maximum-minimum), divide this range into 10 equal sized bins, and then record what fraction of the 200 values fell within each of the bins.</li> 
</li>	
					</ul>
					<!-- Stop edit here -->

				</div>
			</div>

			<div class="section">
				<div class="heading">3. Experiments &amp; Results</div>
				<div class="subsection">
					<div class="heading">3.1 Dataset Description</div>
					<div class="text">

						<!-- Start edit here  -->
						Write something here.
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading">3.2 Discussion</div>
					<div class="text">

						<!-- Start edit here  -->
						Write something here.
						<!-- Stop edit here -->

					</div>
				</div>
			</div>

			<div class="section">
				<div class="heading">4. Conclusions</div>
				<div class="subsection">
					<div class="heading">4.1 Summary</div>
					<div class="text">

						<!-- Start edit here  -->
						 <ul>
						   <li>Smart phone can be used to perform activity recognition simply by keeping it in once's pocket.</li>
						   <li>We further showed that our recognition can detect activities independent of smart phone's position.</li>
						   <li>We further showed that activity recognition can be highly accurate,with most activities being recognised correctly over 90% of the time.</li>
						   <li>In addition, these activities can be recognised quickly since each example is generated from only 10 second worth of data.</li>
						 </ul>
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading">4.2 Future Extensions</div>
					<div class="text">

						<!-- Start edit here  -->
						For future work,we plan to extend our activity recognition task in several ways.First,we intend to recognize additional activities,such as bicycling or sleeping.Second we would like to collect data from more users of various ages.Third we plan to extract more features that could better discriminate different activities.
						<!-- Stop edit here -->

					</div>
				</div>
			</div>

		</div>
	</body>
</html>
